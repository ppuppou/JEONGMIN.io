import pandas as pd
from pathlib import Path
import joblib
import numpy as np
try:
    # ê¶Œì¥ ê²½ë¡œ
    from hdbscan.prediction import approximate_predict
except Exception:
    import hdbscan
    approximate_predict = getattr(hdbscan, "approximate_predict")

try:
    from scipy.stats import chi2
    _HAS_CHI2 = True
except Exception:
    chi2 = None
    _HAS_CHI2 = False

# ================================
# ğŸ”´ ì´ìƒì¹˜ íŒë³„ ì„ê³„ê°’(ìš”êµ¬ì‚¬í•­)
# ================================
ANOMALY_PROBA_THRESHOLD = 0.9  # anom_proba >= 0.9 ì´ë©´ ì´ìƒì¹˜ë¡œ íŒì •

# ================================
# ğŸ“ ë°ì´í„° ë¡œë”© (ì¸ì½”ë”© ì•ˆì „)
# ================================
app_dir = Path(__file__).parent

def load_csv_robustly(file_path: Path) -> pd.DataFrame:
    for enc in ("utf-8-sig", "utf-8", "cp949"):
        try:
            return pd.read_csv(file_path, encoding=enc)
        except UnicodeDecodeError:
            continue
    raise UnicodeDecodeError("read", b"", 0, 1, f"Cannot open {file_path.name} with utf-8/cp949")

try:
    train_df = load_csv_robustly(app_dir / "data/train_df.csv")
    test_df = load_csv_robustly(app_dir / "data/test.csv")
    test_label_df = load_csv_robustly(app_dir / "data/test_label.csv")
    print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    print(f"âš ï¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    train_df, test_df, test_label_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

# ================================
# ğŸ“ ë¶ˆëŸ‰(ì´ì§„) ë¶„ë¥˜ ëª¨ë¸ ë¡œë”©
# ================================
try:
    model_dict = joblib.load(app_dir / "data/final_model.pkl")
    defect_model = model_dict["model"]
    feature_cols = model_dict["feature_names"]
    defect_threshold = 0.8346291545170944
    print("âœ… ë¶ˆëŸ‰íƒì§€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    defect_model, feature_cols, defect_threshold = None, [], 0.5

# ================================
# âœ¨ ë°ì´í„° ì „ì²˜ë¦¬(í•„ìš” í•„ë“œë§Œ)
# ================================
if "Unnamed: 0" in train_df.columns:
    train_df = train_df.drop(columns=["Unnamed: 0"])

if "registration_time" in test_df.columns:
    try:
        test_df["registration_time"] = pd.to_datetime(test_df["registration_time"])
        test_df["hour"] = test_df["registration_time"].dt.hour
    except Exception as e:
        print(f"âš ï¸ 'registration_time' ì²˜ë¦¬ ê²½ê³ : {e}")

# ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì›ì²œ
streaming_df = test_df.copy()

# ================================
# â±ï¸ RealTimeStreamer
# ================================
class RealTimeStreamer:
    def __init__(self, data: pd.DataFrame):
        self.full_data = data.reset_index(drop=True).copy()
        self.current_index = 0

    def get_next_batch(self, batch_size: int = 1):
        if self.current_index >= len(self.full_data):
            return None
        end = min(self.current_index + batch_size, len(self.full_data))
        batch = self.full_data.iloc[self.current_index:end].copy()
        self.current_index = end
        return batch

    def get_current_data(self):
        if self.current_index == 0:
            return pd.DataFrame()
        return self.full_data.iloc[: self.current_index].copy()

    def reset_stream(self):
        self.current_index = 0

    def progress(self) -> float:
        return 0.0 if len(self.full_data) == 0 else (self.current_index / len(self.full_data)) * 100

# ================================
# ğŸ“ HDBSCAN Router ë¡œë“œ
# ================================
try:
    # êµ¬ì¡°: {'segment_col','preprocessor','global', 'segments'...}
    hdb_router = joblib.load(app_dir / "data/hdbscan_router.pkl")
    hdb_seg_col = hdb_router.get("segment_col", "mold_code")
    hdb_prep = hdb_router.get("preprocessor", None)
    hdb_global = hdb_router.get("global", {})
    hdb_segments = hdb_router.get("segments", {})
    print("âœ… HDBSCAN Router ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    print(f"âš ï¸ HDBSCAN Router ë¡œë“œ ì‹¤íŒ¨: {e}")
    hdb_router, hdb_seg_col, hdb_prep, hdb_global, hdb_segments = None, None, None, None, {}

ANOM_BUNDLE         = hdb_router
ANOM_PREPROCESSOR   = hdb_prep
ANOM_FEATURE_ORDER  = (hdb_global or {}).get("feature_order", [])

# ë°©ì–´: feature_orderê°€ ë¹„ì–´ìˆìœ¼ë©´ (ê±°ì˜ ì—†ê² ì§€ë§Œ) í•™ìŠµì— ì“°ì¸ ì›ë³¸ ì¹¼ëŸ¼ìœ¼ë¡œ ëŒ€ì²´
# -> ê°€ì¥ ì•ˆì „í•œ ê±´ train_df ì»¬ëŸ¼ê³¼ êµì§‘í•©ì„ ì“°ëŠ” ê²ƒ
if not ANOM_FEATURE_ORDER:
    try:
        # train_df ê°€ shared.py ì•ˆì—ì„œ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆë‹¤ë©´:
        ANOM_FEATURE_ORDER = [c for c in train_df.columns]
    except Exception:
        pass  # ìµœì†Œí•œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆê²Œë§Œ ë³´ì¥í•˜ë©´ ë¨

def anomaly_transform(df_raw: pd.DataFrame) -> np.ndarray:
    """
    ëª¨ë¸ê³¼ 'ë™ì¼' ì „ì²˜ë¦¬(ColumnTransformer)ë¥¼ ì ìš©í•´ì„œ
    raw DataFrame -> ì „ì²˜ë¦¬ í›„ numpy array ë¡œ ë³€í™˜.
    - ê²°ì¸¡ì¹˜ëŠ” KNNImputer/Most Frequent ë“±, í•™ìŠµ ë•Œì™€ ê°™ì€ ê·œì¹™ìœ¼ë¡œ ì²˜ë¦¬
    - ë²”ì£¼ëŠ” OrdinalEncoder(unknown_value=-1) ê·œì¹™ìœ¼ë¡œ ì²˜ë¦¬
    - ì¹¼ëŸ¼ ìˆœì„œëŠ” hdb_global['feature_order']ë¥¼ ë”°ë¥¸ë‹¤
    """
    if ANOM_PREPROCESSOR is None:
        raise RuntimeError("ANOM_PREPROCESSORê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    if not ANOM_FEATURE_ORDER:
        raise RuntimeError("ANOM_FEATURE_ORDER(=feature_order)ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    # ëˆ„ë½ ì¹¼ëŸ¼ ìë™ ìƒì„±(=NaN) + ìˆœì„œ ë§ì¶”ê¸°
    X = df_raw.reindex(columns=ANOM_FEATURE_ORDER)

    # ColumnTransformerê°€ ì•Œì•„ì„œ ìˆ«ì/ë²”ì£¼ íŒŒì´í”„ë¼ì¸ì„ ì ìš©í•˜ê³  ê²°ì¸¡ë„ ì²˜ë¦¬
    Z = ANOM_PREPROCESSOR.transform(X)
    return Z

def get_preproc_feature_names_out() -> list:
    """
    ì „ì²˜ë¦¬ ê²°ê³¼ì˜ ì¹¼ëŸ¼ëª…(ìŠ¤ì¼€ì¼/ì¸ì½”ë”© í›„ í”¼ì²˜ëª…)ì„ ë°˜í™˜.
    verbose_feature_names_out=False ë¡œ ì €ì¥ë˜ì—ˆë‹¤ë©´ ì›ë˜ ì´ë¦„ê³¼ 1:1 ë§¤í•‘ë¨.
    """
    try:
        return list(ANOM_PREPROCESSOR.get_feature_names_out(ANOM_FEATURE_ORDER))
    except Exception:
        # ë²„ì „ì— ë”°ë¼ ì—†ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì•ˆì „íŒ
        Z0 = anomaly_transform(pd.DataFrame([ {c: np.nan for c in ANOM_FEATURE_ORDER} ]))
        return [f"feat_{i}" for i in range(Z0.shape[1])]

# (ì„ íƒ) ì„¸ê·¸ë¨¼íŠ¸ ì»¬ëŸ¼ì„ ì•±ì—ì„œ ì“°ê³  ì‹¶ìœ¼ë©´ ê°™ì´ export
ANOM_SEGMENT_COL = hdb_seg_col
HDB_SEGMENTS     = hdb_segments


# ================================================================
# ğŸ”´ [ìˆ˜ì •ë¨] ğŸ“Š ì°¸ì¡° ë¶„í¬(ì ìˆ˜/ê±°ë¦¬) ì¤€ë¹„ (ë¯¸ë¦¬ ê³„ì‚°ëœ íŒŒì¼ ë¡œë“œ)
# ================================================================
#
# ê¸°ì¡´ì˜ ë¬´ê±°ìš´ ê³„ì‚° ë¡œì§(train_df.copy(), transform, predict ë“±)ì„
# ëª¨ë‘ ì œê±°í•˜ê³ , 'create_reference_stats.py'ë¡œ ë¯¸ë¦¬ ìƒì„±í•œ
# 'hdbscan_reference_stats.pkl' íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
#

def _get_feature_order(preprocessor, bundle) -> list:
    """ì „ì²˜ë¦¬ê¸°ì—ì„œ í”¼ì²˜ ìˆœì„œ ì¶”ì¶œ (ì´ í•¨ìˆ˜ëŠ” predict_anomalyì—ì„œ í•„ìš”í•˜ë¯€ë¡œ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤)"""
    try:
        if "feature_order" in bundle:
            return list(bundle["feature_order"])
        num_cols, cat_cols = [], []
        for name, _, cols in getattr(preprocessor, "transformers_", []):
            if name == "num":
                num_cols = list(cols)
            elif name == "cat":
                cat_cols = list(cols)
        return num_cols + cat_cols
    except Exception:
        return []

try:
    # 1ë‹¨ê³„ì—ì„œ ìƒì„±í•œ .pkl íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    stats_path = app_dir / "data/hdbscan_reference_stats.pkl"
    stats_bundle = joblib.load(stats_path)
    
    _ANOM_SCORE_REF = stats_bundle.get("ANOM_SCORE_REF")
    _RD2_REF = stats_bundle.get("RD2_REF")
    _N_NUM_COLS = stats_bundle.get("N_NUM_COLS", 0)
    
    if _RD2_REF is not None:
        print(f"âœ… RD2 ì°¸ì¡° ë¡œë“œ ì™„ë£Œ (n={len(_RD2_REF)})")
    if _ANOM_SCORE_REF is not None:
        print(f"âœ… Score ì°¸ì¡° ë¡œë“œ ì™„ë£Œ (n={len(_ANOM_SCORE_REF)})")

except Exception as e:
    print(f"âš ï¸ 'hdbscan_reference_stats.pkl' ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("   -> ğŸ”´ ì¤‘ìš”: ë°°í¬ ì „, ë¡œì»¬ì—ì„œ 'create_reference_stats.py' ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
    _ANOM_SCORE_REF, _RD2_REF, _N_NUM_COLS = None, None, 0
    
# ================================================================
# ğŸ”´ [ìˆ˜ì • ì™„ë£Œ]
# ================================================================

# ================================
# ğŸ” ì‹¤ì‹œê°„ ì´ìƒì¹˜ íƒì§€ (ì„ê³„ê°’ 0.9 ì‚¬ìš©)
# ================================
def predict_anomaly(df: pd.DataFrame) -> pd.DataFrame | None:
    """HDBSCAN Router ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€.
    ë°˜í™˜: id, anomaly_status(0/1), prob(strength), anom_proba
    """
    if hdb_router is None or hdb_prep is None or df.empty:
        return None

    df_in = df.copy().reset_index(drop=False)  # ì› ì¸ë±ìŠ¤ ë³´ì¡´
    idx_col = df_in.columns[0]

    # ì„¸ê·¸ë¨¼íŠ¸ í‚¤
    if hdb_seg_col and (hdb_seg_col in df_in.columns):
        seg_series = df_in[hdb_seg_col].astype("object")
        seg_series = seg_series.mask(pd.isna(seg_series), "NA")
        seg_keys = seg_series.apply(lambda x: f"{hdb_seg_col}={x}")
    else:
        seg_keys = pd.Series(["__global__"] * len(df_in))

    out_anom = np.zeros(len(df_in), dtype=int)
    out_prob = np.zeros(len(df_in), dtype=float)        # membership strength
    out_anom_proba = np.zeros(len(df_in), dtype=float)  # ìš°ë¦¬ê°€ ê·¸ë¦´ ê°’

    for key, idx in seg_keys.groupby(seg_keys).groups.items():
        bundle = hdb_segments.get(key, None) or hdb_global
        feat_order = _get_feature_order(hdb_prep, bundle)
        if not feat_order:
            continue

        Xg = df_in.loc[idx].copy()
        for c in feat_order:
            if c not in Xg.columns:
                Xg[c] = 0
        Xg = Xg[feat_order]

        try:
            Xp = hdb_prep.transform(Xg)
            pca = bundle.get("pca", None)
            Z = pca.transform(Xp) if pca is not None else Xp

            labels, strengths = approximate_predict(bundle["hdb"], Z)
            strengths = np.asarray(strengths).astype(float).reshape(-1)
            anom_scores = np.where(np.asarray(labels) == -1, 1.0, 1.0 - strengths)

            # anom_proba ê³„ì‚°: ì°¸ì¡° ì ìˆ˜ë¶„í¬ ê¸°ì¤€ì˜ ëˆ„ì ë°±ë¶„ìœ„(ì—†ìœ¼ë©´ min-max)
            if _ANOM_SCORE_REF is not None and len(_ANOM_SCORE_REF) > 0:
                pos = np.searchsorted(_ANOM_SCORE_REF, anom_scores, side="right")
                anom_proba = pos / len(_ANOM_SCORE_REF)
            else:
                s_min, s_ptp = np.min(anom_scores), np.ptp(anom_scores)
                anom_proba = (anom_scores - s_min) / (s_ptp + 1e-9) if s_ptp > 0 else np.zeros_like(anom_scores)

        except Exception as e:
            # Fallback: ê±°ë¦¬ ê¸°ë°˜
            print(f"â„¹ï¸ Fallback(seg={key}): {e}")
            strengths = np.zeros(len(idx), dtype=float)
            if hasattr(hdb_prep, "transformers_") and any(t[0] == "num" for t in hdb_prep.transformers_):
                rd2 = np.sum(np.square(Xp[:, :_N_NUM_COLS]), axis=1) if _N_NUM_COLS > 0 else np.zeros(len(idx))
                if _RD2_REF is not None and len(_RD2_REF) > 0:
                    pos = np.searchsorted(_RD2_REF, rd2, side="right")
                    anom_proba = pos / len(_RD2_REF)
                elif _HAS_CHI2 and _N_NUM_COLS > 0:
                    anom_proba = chi2.cdf(rd2, df=_N_NUM_COLS)
                else:
                    anom_proba = (rd2 - rd2.min()) / (rd2.ptp() + 1e-9) if rd2.ptp() > 0 else np.zeros_like(rd2)
            else:
                anom_proba = np.zeros(len(idx))

        # ğŸ”´ ì„ê³„ê°’ ì ìš© (ì—¬ê¸°ê°€ í•µì‹¬)
        preds = (anom_proba >= ANOMALY_PROBA_THRESHOLD).astype(int)

        out_anom[list(idx)] = preds
        out_prob[list(idx)] = strengths
        out_anom_proba[list(idx)] = anom_proba

    return pd.DataFrame({
        "id": df_in[idx_col].values,
        "anomaly_status": out_anom,
        "prob": out_prob,           # membership strength (ì •ìƒì¼ìˆ˜ë¡ í¼)
        "anom_proba": out_anom_proba
    })
